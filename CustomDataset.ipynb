{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae6cd124-e868-4b50-a721-90827ae08914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47d4b791-5548-40a7-acf5-82191ea56a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexNet = models.alexnet(weights=\"IMAGENET1K_V1\", progress=True)\n",
    "AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1131ab9-7a73-4bc0-b233-8545350251c2",
   "metadata": {},
   "source": [
    "# Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf2aa552-172f-40c8-ae05-1567890ff49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', \n",
    "    'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', \n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4e4781a-41e8-4dcb-9ca0-4648d89cfc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset():\n",
    "    def __init__(self, root, transform, img_set='trainval', num_classes=20):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.img_set = img_set\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.annotation_path = os.path.join(self.root, f'PASCAL_VOC_{self.img_set}', 'VOCdevkit', 'VOC2007', 'Annotations')\n",
    "        self.img_path = os.path.join(self.root, f'PASCAL_VOC_{self.img_set}', 'VOCdevkit', 'VOC2007', 'JPEGimages')\n",
    "\n",
    "        self.annotations = [os.path.join(self.annotation_path, xml) for xml in os.listdir(self.annotation_path)]\n",
    "        self.images = [os.path.join(self.img_path, xml) for xml in os.listdir(self.img_path)]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    \n",
    "    def parse_xml_boxes(self, root):\n",
    "        bboxes = []\n",
    "        for obj in root.findall('object'):\n",
    "            cls_name = obj.find('name').text\n",
    "            if cls_name in VOC_CLASSES:\n",
    "                cls_idx = VOC_CLASSES.index(cls_name)\n",
    "                xmin = int(obj.find('bndbox').find('xmin').text)\n",
    "                xmax = int(obj.find('bndbox').find('xmax').text)\n",
    "                ymin = int(obj.find('bndbox').find('ymin').text)\n",
    "                ymax = int(obj.find('bndbox').find('ymax').text)\n",
    "            bboxes.append([xmin, xmax, ymin, ymax, cls_idx])\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        annotation_path = self.annotations[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        bboxes = parse_xml_boxes(root)\n",
    "\n",
    "        image_width = int(root.find('size/width').text)\n",
    "        image_height = int(root.find('size/height').text)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8b6854b-baa1-4837-a185-98eca132341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "VOC_VAL = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
    "\n",
    "img_transform = v2.Compose([\n",
    "    v2.Resize((448, 448)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=VOC_VAL[0], std=VOC_VAL[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2c25732-de2b-4ad7-8e64-720458f61552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5011\n",
      "4952\n"
     ]
    }
   ],
   "source": [
    "root = '/Users/h383kim/pytorch/data'\n",
    "train_dataset = CustomDataset(root, img_transform, 'trainval', 20)\n",
    "test_dataset = CustomDataset(root, img_transform, 'test', 20)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0ac342b-74dd-426a-85bb-ec8efe5d8759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7dacc-d6e6-4fbe-bed1-70adf486c2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d8d3c6a-b413-4b4d-a6d7-fa2acf6495cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataset.transform = transforamtion\n",
    "test_dataset.transform = transforamtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d1d947f-c719-40c0-8a4d-c936f3ec4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
